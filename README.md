## Distilling Knowledge from Reader to Retriever

Implementation of the retriever distillation procedure as outlined in the paper <a href="https://arxiv.org/abs/2012.04584">Distilling Knowledge from Reader to Retriever</a> in Pytorch. They propose to train the retriever using the cross attention scores as pseudo-labels. SOTA on QA.

## Citations

```bibtex
@misc{izacard2020distilling,
    title={Distilling Knowledge from Reader to Retriever for Question Answering}, 
    author={Gautier Izacard and Edouard Grave},
    year={2020},
    eprint={2012.04584},
    archivePrefix={arXiv},
    primaryClass={cs.CL}
}
```
